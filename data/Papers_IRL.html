<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title> IRL: Inverse Reinforcement Learning</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
<style>
.container {
  display: flex;
  flex-wrap: wrap;
}
.column {
  flex: 0 25%; /* 幅を25%に設定して4列に */
  box-sizing: border-box;
  padding: 10px;
}
</style>

    </head>
    <body>
        <h1><center> IRL: Inverse Reinforcement Learning (逆強化学習)</center></h1>
<a href="#latest">最新項目へ</a><br>

<!--●--> <h3>Algorithms for inverse reinforcement learning (2000)<br><span style="color:blue;">逆強化学習のアルゴリズム</span></h3>

<p>
IRLの基礎論文。
</p>
<p>
<a href="https://ai.stanford.edu/~ang/papers/icml00-irl.pdf">論文はこちら</a>
</p>


<!--●--> <h3>Generative adversarial imitation learning (2016)<br><span style="color:blue;">敵対的生成模倣学習</span></h3>

<p>
Generative Adversarial Imitation Learning (GAIL) フレームワークを導入し、GANとIRLの関連性を確立した基礎的な論文。
</p>
<p>
<a href="https://arxiv.org/abs/1606.03476">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/1606.03476v1">要約はこちら</a><br>
</p>

<!--●--> <h3>nverse Reinforcement Learning without Reinforcement Learning (2023)<br><span style="color:blue;">強化学習なしの逆強化学習</span></h3>

<p>
逆強化学習（IRL）は、熟練者のデモンストレーションを合理化する報酬関数の学習を目的とする、模倣学習のための強力な手法群である。従来のIRL手法には計算上の弱点があり、難しい強化学習（RL）問題をサブルーチンとして繰り返し解く必要があった。 模倣学習というより容易な問題を、より困難なRL問題を繰り返し解く問題に縮約した。理論上は指数関数的な高速化を実現する。実際に、連続制御タスクにおいて従来技術を大幅に高速化できることが分かった。
</p>
<p>
<a href="https://arxiv.org/abs/2303.14623v4">論文はこちら</a>
</p>

<!--●--> <h3 id="latest">Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities (2025)<br><span style="color:blue;">逆強化学習と大規模言語モデルの追加学習の融合：基礎、進展、そして機会</span></h3>

<p>
大規模言語モデル（LLM）の生成を、報酬関数が事前に定義されていないマルコフ決定過程（MDP\R）として捉え、逆強化学習（IRL）がLLMの学習後調整とアライメント(人間の価値観に合うように調整すること)にどのように適用されるかを包括的にレビューしたチュートリアル。 RLHFやDPOといった手法がIRLの一種であることを示している。
</p>
<p>
<a href="https://arxiv.org/abs/2507.13158">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/2507.13158v1">要約はこちら</a><br>
</p>

<a href="#">トップに戻る</a>

<!--
<br>テンプレート<br>

<!--●-- <h3><br><span style="color:blue;"></span></h3>

<p>
</p>
<p>
<a href="">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/">要約はこちら</a><br>
</p>



<div class="styleBullet">
<ul><li>

</li><br><li>

</li></ul></div>

-->

    </body>
</html>