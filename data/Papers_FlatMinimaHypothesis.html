<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>Flat Minima Hypothesis</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
<style>
.container {
  display: flex;
  flex-wrap: wrap;
}
.column {
  flex: 0 25%; /* 幅を25%に設定して4列に */
  box-sizing: border-box;
  padding: 10px;
}
</style>

    </head>
    <body>
        <h1><center>Flat minima hypothesis (平坦な最適解空間仮説)</center></h1>
<a href="#latest">最新項目へ</a><br>

<h3>Flat Minima (1997)<br><span style="color:blue;">平坦な最小値</span></h3>

<p>
平坦な最小値が優れた汎化性能につながることを、情報理論（最小記述長原理、MDL）とベイズ主義的な観点から初めて詳細に論じた。
</p>
<p>
<a href="https://www.bioinf.jku.at/publications/older/3304.pdf">論文はこちら</a><br>
</p>


<h3>Entropy-SGD: biasing gradient descent into wide valleys (2016)<br><span style="color:blue;">エントロピーSGD：勾配降下法を広い谷に偏らせる</span></h3>

<p>
実際に平坦な最小値へとモデルを誘導する具体的な最適化アルゴリズムを提案した
</p>
<p>
<a href="https://wordpress.cs.vt.edu/optml/2018/04/29/entropy-sgd-biasing-gd-into-wide-valleys/">論文はこちら</a><br>
</p>


<h3>Sharp Minima Can Generalize For Deep Nets (2017)<br><span style="color:blue;">鋭い極小値はディープネットにも一般化できる</span></h3>

<p>
モデルのパラメータを再パラメータ化（重みのスケーリングなど）することで、汎化性能を変えずに最小値の鋭さを人為的に操作できることを示し、この仮説に疑問を投げかけた。
</p>
<p>
<a href="https://arxiv.org/abs/1703.04933">論文はこちら</a><br>
</p>


<h3>Relative Flatness and Generalization (2020)<br><span style="color:blue;">相対的な平坦性と一般化</span></h3>

<p>
再パラメータ化不変な相対平坦性の尺度を導入した。
</p>
<p>
<a href="https://arxiv.org/abs/2001.00939">論文はこちら</a><br>
</p>


<h3 id="latest">Flatness is Necessary, Neural Collapse is Not: Rethinking Generalization via Grokking (2025)<br><span style="color:blue;">平坦性は必要だが、ニューラルコラプスは不要：グロッキングによる汎化の再考</span></h3>

<p>
ニューラルコラプスと損失ランドスケープの平坦性は理論的にも経験的にも汎化と結び付けられてきた。しかし、どちらの現象の因果関係も依然として不明だった。グロッキングを用いてこれらの疑問を解明した。
ニューラルコラプスと相対的平坦性はどちらも汎化の開始時に出現するが、平坦性のみが一貫して汎化を予測できることが分かった。
</p>
<p>
<a href="https://arxiv.org/abs/2509.17738">論文はこちら</a><br>
</p>

<a href="#">トップに戻る</a>

<!--
<br>テンプレート<br>

<h3><br><span style="color:blue;"></span></h3>

<p>
</p>
<p>
<a href="">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/">要約はこちら</a><br>
</p>

-->

    </body>
</html>