<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>AI Alignment</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
<style>
.container {
  display: flex;
  flex-wrap: wrap;
}
.column {
  flex: 0 25%; /* 幅を25%に設定して4列に */
  box-sizing: border-box;
  padding: 10px;
}
</style>

    </head>
    <body>
        <h1><center>AI Alignment (AIアライメント)</center></h1>
<a href="#DPO">最新項目へ</a><br>

<h2>近似最適近似強化学習 (2002)</h2>
(Approximately optimal approximate reinforcement learning)

<p>
代理目的関数の理論的基盤となる保守的方策反復 (CPI) を導入した。
</p>
<p>
<a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf">論文はこちら</a><br>
</p>

<h2>APRIL: 能動的な選好学習に基づく強化学習 (2012)</h2>
(April: Active preference learning-based reinforcement learning)

<p>
好みから報酬関数を学習し、その後、強化学習を用いてそれを最適化するというアプローチを提唱した。<br>
『専門家の好みを利用して近似的な方策リターンを学習することで、エージェントが直接的な方策探索を実現できる･･･』
</p>
<p>
<a href="https://arxiv.org/abs/1208.0984">論文はこちら</a><br>
</p>

<h2>超知能と人間の利益の整合：技術研究アジェンダ (2014)</h2>
(Aligning Superintelligence with Human Interests:A Technical Research Agenda )

<p>
FAI(Friendly AI)という目標を達成するための研究分野に<strong><span style="color:magenta;">AI Alignment という用語を定着させた</span></strong>。<br>
<br>
『人工知能の進歩がこのまま続けば、AIシステムは最終的に一般的な推論能力において人間を超えるだろう。「事実上あらゆる分野で人間の最高の脳よりも賢い」という意味で「超知能」を持つシステムは、人類に計り知れない影響を与える可能性がある（Bostrom 2014）。人間の知能によって環境を制御するためのツールや戦略を開発できるようになったように、超知能システムは、制御を行うための独自のツールや戦略を開発できる可能性が高い（Muehlhauser and Salamon 2012）。こうした可能性を考慮すると、人間の一般的な知能レベルを超えるAIシステム、あるいはそのようなシステムの構築を容易にするAIシステムの開発には、慎重を期すことが不可欠である･･･』
</p>
<p>
<a href="https://intelligence.org/files/obsolete/TechnicalAgenda%5Bold%5D.pdf">論文はこちら</a><br>
</p>

<h2>信頼領域ポリシーの最適化 (2015)</h2>
(Trust Region Policy Optimization)

<p>
『単調な改善が保証されたポリシー最適化のための反復手順について述べる。理論的に正当化された手順に複数の近似を行うことで、<strong><span style="color:magenta;">信頼領域ポリシー最適化 (TRPO: Trust Region Policy Optimization) と呼ばれる実用的なアルゴリズムを開発した</span></strong>。』
</p>
<p>
<a href="https://arxiv.org/abs/1502.05477">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/1502.05477">要約はこちら</a><br>
</p>


<h2>協力的逆強化学習 (2016)</h2>
(Cooperative Inverse Reinforcement Learning)
<p>
エージェントが相互作用を通じて人間の目標を学習する問題を形式的にモデル化した。<br>
<br>
『自律システムが人間に役立ち、不当なリスクをもたらさないためには、その行動が人間にとっての価値の最大化に寄与するように、環境内の人間の価値観と自らの価値観を整合させる必要がある。本稿では、この価値観整合問題の正式な定義として、協力的逆強化学習（CIRL）を提案する･･･』
</p>
<p>
<a href="https://arxiv.org/abs/1606.03137">論文はこちら</a><br>
</p>

<h2>人間の好みからの深層強化学習 (2017)</h2>
(Deep reinforcement learning from human preferences)

<p>
人間からのフィードバックによる強化学習という概念を導入した基礎的な論文。<br>
<strong><span style="color:magenta;">人間のフィードバックが手作業で設計された報酬関数よりも優れた報酬形を提供できる可能性を示唆しており,従来の報酬設計が非現実的である実世界の課題に強化学習を適用する新たな可能性を開いた</span></strong>。
</p>
<center><img src="images/RLHF.svg"></center>
<p>
<a href="https://arxiv.org/abs/1706.03741">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/1706.03741">要約はこちら</a><br>
</p>

<h2>近似ポリシー最適化アルゴリズム (2017)</h2>
(Proximal Policy Optimization Algorithms)

<p>TRPO (Trust Region Policy Optimization)の複雑さを改善しつつ、TRPOと同等かそれ以上の性能を達成でき、実装がよりシンプルで汎用的な手法として<strong><span style="color:magenta;">PPOを提案した</span></strong>。
</p>
<p>
<a href="https://arxiv.org/abs/1707.06347">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/1707.06347">要約はこちら</a><br>
</p>


<h2>人間からのフィードバックによる要約学習 (2020)</h2>
(Learning to summarize from human feedback)

<p>
人間の好みを満たすようにモデルを直接訓練することが劇的に優れた結果を生み出すことを示している。
</p>
<center><img src="images/RLHF2.svg"></center>
<p>
<a href="https://arxiv.org/abs/2009.01325">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/2009.01325v3">要約はこちら</a><br>
</p>


<h2>アライメントの実験室としての一般言語アシスタント (2021)</h2>
(A general language assistant as a laboratory for alignment)

<p>
有用性、正直さ、無害さに焦点を当てたアライメント評価のフレームワーク
</p>
<p>
<a href="https://arxiv.org/abs/2112.00861">論文はこちら</a><br>
</p>

<h2 id="RLHF">人間からのフィードバックによる指示追従言語モデルの訓練 (2022)</h2>
(Training language models to follow instructions with human feedback)

<p>
多段階の人間からのフィードバックによる強化学習（<strong><span style="color:magenta;">RLHF</span></strong>:Reinforcement Learning from Human Feedback）パイプラインを確立した
</p>
<p>
<a href="https://arxiv.org/abs/2203.02155">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/2203.02155v1">要約はこちら</a><br>
</p>
<center><img src="images/RLHF3.svg"></center>

<h2 id="DPO">ダイレクト選好最適化：あなたの言語モデルは実は報酬モデルである (2023)</h2>
(Direct preference optimization: Your language model is secretly a reward model)

<p>
人間からのフィードバックによる強化学習（RLHF）の複雑さを回避し、大規模言語モデルを人間の好みに合わせてファインチューニングする手法(<strong><span style="color:magenta;">DPO</span></strong>)を導入した。
</p>
<p>
<a href="https://arxiv.org/abs/2305.18290">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/2305.18290v3">要約はこちら</a><br>
</p>
<center><img src="images/DPO.svg"></center>

<a href="#">トップに戻る</a>

<!--
<br>テンプレート<br>

<h2></h2>
()

<p>
</p>
<p>
<a href="">論文はこちら</a><br>
<a href="">要約はこちら</a><br>
</p>

-->

    </body>
</html>