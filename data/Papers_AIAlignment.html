<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>AI Alignment</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
<style>
.container {
  display: flex;
  flex-wrap: wrap;
}
.column {
  flex: 0 25%; /* 幅を25%に設定して4列に */
  box-sizing: border-box;
  padding: 10px;
}
</style>

    </head>
    <body>
        <h1><center>AI Alignment (AIアライメント)</center></h1>
<a href="#latest">最新項目へ</a><br>

<h3>Approximately optimal approximate reinforcement learning (2002)<br><span style="color:blue;">近似最適近似強化学習</span></h3>

<p>
代理目的関数の理論的基盤となる保守的方策反復 (CPI) を導入した。
</p>
<p>
<a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf">論文はこちら</a><br>
</p>

<h3>April: Active preference learning-based reinforcement learning (2012)<br><span style="color:blue;">APRIL: 能動的な選好学習に基づく強化学習</span></h3>

<p>
好みから報酬関数を学習し、その後、強化学習を用いてそれを最適化するというアプローチを提唱した。<br>
『専門家の好みを利用して近似的な方策リターンを学習することで、エージェントが直接的な方策探索を実現できる･･･』
</p>
<p>
<a href="https://arxiv.org/abs/1208.0984">論文はこちら</a><br>
</p>

<h3>Aligning Superintelligence with Human Interests:A Technical Research Agenda (2014)<br><span style="color:blue;">超知能と人間の利益の整合：技術研究アジェンダ</span></h3>

<p>
FAI(Friendly AI)という目標を達成するための研究分野に<strong><span style="color:magenta;">AI Alignment という用語を定着させた</span></strong>。<br>
<br>
『人工知能の進歩がこのまま続けば、AIシステムは最終的に一般的な推論能力において人間を超えるだろう。「事実上あらゆる分野で人間の最高の脳よりも賢い」という意味で「超知能」を持つシステムは、人類に計り知れない影響を与える可能性がある（Bostrom 2014）。人間の知能によって環境を制御するためのツールや戦略を開発できるようになったように、超知能システムは、制御を行うための独自のツールや戦略を開発できる可能性が高い（Muehlhauser and Salamon 2012）。こうした可能性を考慮すると、人間の一般的な知能レベルを超えるAIシステム、あるいはそのようなシステムの構築を容易にするAIシステムの開発には、慎重を期すことが不可欠である･･･』
</p>
<p>
<a href="https://intelligence.org/files/obsolete/TechnicalAgenda%5Bold%5D.pdf">論文はこちら</a><br>
</p>

<h3>Trust Region Policy Optimization (2015)<br><span style="color:blue;">信頼領域ポリシーの最適化</span></h3>

<p>
『単調な改善が保証されたポリシー最適化のための反復手順について述べる。理論的に正当化された手順に複数の近似を行うことで、<strong><span style="color:magenta;">信頼領域ポリシー最適化 (TRPO: Trust Region Policy Optimization) と呼ばれる実用的なアルゴリズムを開発した</span></strong>。』
</p>
<p>
<a href="https://arxiv.org/abs/1502.05477">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/1502.05477">要約はこちら</a><br>
</p>


<h3>Cooperative Inverse Reinforcement Learning (2016)<br><span style="color:blue;">協力的逆強化学習</span></h3>

<p>
エージェントが相互作用を通じて人間の目標を学習する問題を形式的にモデル化した。<br>
<br>
『自律システムが人間に役立ち、不当なリスクをもたらさないためには、その行動が人間にとっての価値の最大化に寄与するように、環境内の人間の価値観と自らの価値観を整合させる必要がある。本稿では、この価値観整合問題の正式な定義として、協力的逆強化学習（CIRL）を提案する･･･』
</p>
<p>
<a href="https://arxiv.org/abs/1606.03137">論文はこちら</a><br>
</p>

<h3 id="RLHF">Deep reinforcement learning from human preferences (2017)<br><span style="color:blue;">人間の好みからの深層強化学習</span></h3>

<p>
人間からのフィードバックによる強化学習という概念を導入した基礎的な論文。<br>
<strong><span style="color:magenta;">人間のフィードバックが手作業で設計された報酬関数よりも優れた報酬形を提供できる可能性を示唆しており,従来の報酬設計が非現実的である実世界の課題に強化学習を適用する新たな可能性を開いた</span></strong>。
</p>
<center><img src="images/RLHF.svg"></center>
<p>
<a href="https://arxiv.org/abs/1706.03741">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/1706.03741">要約はこちら</a><br>
</p>

<h3>Proximal Policy Optimization Algorithms (2017)<br><span style="color:blue;">近似ポリシー最適化アルゴリズム</span></h3>

<p>TRPO (Trust Region Policy Optimization)の複雑さを改善しつつ、TRPOと同等かそれ以上の性能を達成でき、実装がよりシンプルで汎用的な手法として<strong><span style="color:magenta;">PPOを提案した</span></strong>。
</p>
<p>
<a href="https://arxiv.org/abs/1707.06347">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/1707.06347">要約はこちら</a><br>
</p>


<h3>Learning to summarize from human feedback (2020)<br><span style="color:blue;">人間からのフィードバックによる要約学習</span></h3>

<p>
人間の好みを満たすようにモデルを直接訓練することが劇的に優れた結果を生み出すことを示している。
</p>
<center><img src="images/RLHF2.svg"></center>
<p>
<a href="https://arxiv.org/abs/2009.01325">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/2009.01325v3">要約はこちら</a><br>
</p>


<h3>A general language assistant as a laboratory for alignment (2021)<br><span style="color:blue;">アライメントの実験室としての一般言語アシスタント</span></h3>

<p>
有用性、正直さ、無害さに焦点を当てたアライメント評価のフレームワーク
</p>
<p>
<a href="https://arxiv.org/abs/2112.00861">論文はこちら</a><br>
</p>

<h3>Training language models to follow instructions with human feedback (2022)<br><span style="color:blue;">人間からのフィードバックによる指示追従言語モデルの訓練</span></h3>

<p>
多段階の人間からのフィードバックによる強化学習（<strong><span style="color:magenta;">RLHF</span></strong>:Reinforcement Learning from Human Feedback）パイプラインを確立した
</p>
<p>
<a href="https://arxiv.org/abs/2203.02155">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/2203.02155v1">要約はこちら</a><br>
</p>
<center><img src="images/RLHF3.svg"></center>

<h3 id="DPO">Direct preference optimization: Your language model is secretly a reward model (2023)<br><span style="color:blue;">ダイレクト選好最適化：あなたの言語モデルは実は報酬モデルである</span></h3>

<p>
人間からのフィードバックによる強化学習（RLHF）の複雑さを回避し、大規模言語モデルを人間の好みに合わせてファインチューニングする手法(<strong><span style="color:magenta;">DPO</span></strong>)を導入した。
</p>
<p>
<a href="https://arxiv.org/abs/2305.18290">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/2305.18290v3">要約はこちら</a><br>
</p>
<center><img src="images/DPO.svg"></center>

<h3>A General Theoretical Paradigm to Understand Learning from Human Preferences (2023)<br><span style="color:blue;">人間の選好からの学習を理解するための汎用的な理論的パラダイム</span></h3>

<p>
人間のフィードバックからの強化学習 (RLHF) と直接選好最適化 (DPO) を統合する一般的な理論的フレームワークであるΨ選好最適化 (ΨPO) を開発した。
</p>
<p>
<a href="https://arxiv.org/abs/2310.12036">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/2310.12036">要約はこちら</a><br>
</p>


<h3 id="latest">Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities (2025)<br><span style="color:blue;">逆強化学習と大規模言語モデルの追加学習の融合：基礎、進展、そして機会</span></h3>

<p>
『逆強化学習（IRL）の視点からLLMのアライメントを理解するための包括的なフレームワークを提示する』
</p>
<p>
<a href="https://arxiv.org/abs/2507.13158">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/2507.13158v1">要約はこちら</a><br>
</p>


<a href="#">トップに戻る</a>

<!--
<br>テンプレート<br>

<h3><br><span style="color:blue;"></span></h3>

<p>
</p>
<p>
<a href="">論文はこちら</a><br>
<a href="">要約はこちら</a><br>
</p>

-->

    </body>
</html>