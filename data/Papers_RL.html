<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>RL: Reinforcement Learning (強化学習)</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 20px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
<style>
.container {
  display: flex;
  flex-wrap: wrap;
}
.column {
  flex: 0 25%; /* 幅を25%に設定して4列に */
  box-sizing: border-box;
  padding: 10px;
}
</style>

    </head>
    <body>
        <h1><center>RL: Reinforcement Learning (強化学習)</center></h1>

<h3>モデルレス手法</h3>

<p style="padding-left: 1em;background-color:rgb(226,240,217);">
■価値ベース手法<br>

</p><p style="padding-left: 2em;">
<strong>Q-Learning</strong><br>
　●『Learning from Delayed Rewards (遅延報酬からの学習)』(1989)<br>
　　　Google Scholarで検索するとpdfに辿り着ける(241ページの博士論文, 画像pdf)<br>

</p><p style="padding-left: 4em;">

動物や人間が、複数のステップや時間遅れを経てようやく得られる報酬から、複雑な運動や社会的行動を学習する脳のメカニズムと、それを計算モデル（アルゴリズム）でどのように実現するかを探求している。

</p><p style="padding-left: 2em;">
<strong>SARSA</strong>: <strong>S</strong>tate ⇒ <strong>A</strong>ction ⇒ <strong>R</strong>eward ⇒ <strong>S</strong>tate ⇒ <strong>A</strong>ction<br>
　●『On-Line Q-Learning Using Connectionist Systems (コネクショニストシステムを用いたオンラインQ学習)』(1994)<br>
　　　Google Scholarで検索するとpdfに辿り着ける(21ページ, 画像pdf)<br>


</p><p style="padding-left: 2em;">
<strong>DQN: Deep Q-Network</strong><br>
　●『Playing Atari with Deep Reinforcement Learning (深層強化学習でAtariをプレイする)』(2013)<br>
　　論文は<a href="https://arxiv.org/abs/1312.5602">こちら</a>

</p><p style="padding-left: 4em;">
深層学習 (Deep Learning) と強化学習 (Reinforcement Learning) を組み合わせた最初の成功例の一つとなる画期的な論文。<br>

</p><p style="padding-left: 2em;">
<strong>Monte Carlo</strong><br>
　●『Expected-outcome: A general model of game-playing programs』(1987)<br>
　　　論文は<a href="https://academiccommons.columbia.edu/doi/10.7916/D8TF05DD/download">こちら</a>。(125ページ。OCRは掛けてある）

</p><p style="padding-left: 2em;">
<strong>TD: Temporal Difference</strong><br>
　●『Learning to Predict by the Methods of Temporal Differences (時間的差異の手法による予測学習)』(1988)<br>
　　　論文は<a href="http://incompleteideas.net/papers/sutton-88-with-erratum.pdf">こちら</a>

</p><p style="padding-left: 1em;background-color:rgb(222,235,247);">
■ポリシーベース手法<br>

</p><p style="padding-left: 2em;">
<strong>PPO: Proximal Policy Optimization</strong><br>
　●『Proximal Policy Optimization Algorithms (近接方策最適化アルゴリズム)』(2017)<br>
　　　論文は<a href="https://arxiv.org/abs/1707.06347">こちら</a>, 解説は<a href="https://www.alphaxiv.org/overview/1707.06347v2">こちら</a>　(EN→JAで日本語表示する)

</p><p style="padding-left: 2em;">
<strong>DDPG: Deep Deterministic Policy Gradient</strong><br>
　●『Continuous control with deep reinforcement learning (深層強化学習を使った連続制御)』(2015)<br>
　　論文は<a href="https://arxiv.org/abs/1509.02971">こちら</a>

</p><p style="padding-left: 1em;background-color:rgb(255,242,204);">
■Actor-Critic手法<br>

</p><p style="padding-left: 2em;">
<strong>Actor-Critic</strong><br>
　●『Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems (困難な学習制御問題を解決できるニューロン様適応要素)』(1983)<br>
　　論文は<a href="http://incompleteideas.net/papers/barto-sutton-anderson-83.pdf">こちら</a>

</p><p style="padding-left: 2em;">
<strong>A2C/A3C: Advantage Actor-Critic/Asynchronous Advantage Actor-Critic</strong><br>
　●『Asynchronous Methods for Deep Reinforcement Learning (深層強化学習のための非同期手法)』(2016)<br>
　　論文は<a href="https://arxiv.org/abs/1602.01783">こちら</a><br>
</p>

</p><p style="padding-left: 2em;">
<strong>DDPG: Deep Deterministic Policy Gradient</strong> [再掲]

</p>

<h3>モデルベース手法</h3>

<p style="padding-left: 1em;background-color:rgb(251,229,214);">
■モデルベース手法<br>

</p><p style="padding-left: 2em;">
<strong>MCTS: Monte Carlo Tree Search</strong><br>
　●『Bandit-based Monte-Carlo Planning』(2006)<br>
　　　論文は<a href="http://ggp.stanford.edu/readings/uct.pdf">こちら</a><br>

<br>
<a href="#">トップに戻る</a>

<!--
<br>テンプレート<br>

<!--●-- <h3><br><span style="color:blue;"></span></h3>

<p>
</p>
<p>
<a href="">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/">要約はこちら</a><br>
</p>



<div class="styleBullet">
<ul><li>

</li><br><li>

</li></ul></div>

-->

    </body>
</html>