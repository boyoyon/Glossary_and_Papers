<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>Double Descent</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
<style>
.container {
  display: flex;
  flex-wrap: wrap;
}
.column {
  flex: 0 25%; /* 幅を25%に設定して4列に */
  box-sizing: border-box;
  padding: 10px;
}
</style>

    </head>
    <body>
        <h1><center>Double Descent (二重降下)</center></h1>
<a href="#latest">最新項目へ</a><br>

<h3>Reconciling modern machine learning practice and the classical bias-variance trade-off (2018)<br><span style="color:blue;">現代機械学習の実践とバイアス・バリアンスのトレードオフの両立</span></h3>

<p>
従来のバイアス-バリアンスのトレードオフの概念に異議を唱えた。モデルの複雑さを増すとテスト誤差が増加するという古典的な理論に対し、モデルが過剰パラメータ化された領域ではテスト誤差が再び減少し始めるという、二重降下の現象を実証した。<br>
特に、「補間閾値」（モデルが訓練データを完全に記憶する点）でテスト誤差がピークに達し、それを超えてさらにパラメータを増やし、モデルを大きくする（過剰パラメータ化する）ことでテスト誤差が再び低下していくことを示した。<br>
この研究は、なぜ過剰パラメータ化された大規模な深層学習モデルが、正則化なしでもうまく汎化するのかという、当時の大きな謎を解き明かす鍵となった。
<p>
<a href="https://arxiv.org/abs/1812.11118">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/1812.11118v2">要約はこちら</a><br>
</p>


<h3>Deep Double Descent: Where Bigger Models and More Data Hurt (2019)<br><span style="color:blue;">深層ダブルディセント：より大きなモデルとより多くのデータが有害になる場合</span></h3>

<p>
モデルサイズだけでなく、訓練のエポック数やデータセットサイズを変化させた場合にも二重降下が発生することを、さまざまなニューラルネットワークで実証した。
</p>
<p>
<a href="https://arxiv.org/abs/1912.02292">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/1912.02292v1">要約はこちら</a><br>
</p>

<h3 id="latest">A Brief Prehistory of Double Descent (2020)<br><span style="color:blue;">二重降下の簡単な前史</span></h3>

<p>
1980年代から1990年代にかけての統計学やパターン認識の文献を再調査し、現代の二重降下現象は、まったく新しい発見というわけではなく、過去の研究の延長線上にあることを明らかにした。
</p>
<p>
<a href="https://arxiv.org/abs/2004.04328">論文はこちら</a><br>
</p>


<a href="#">トップに戻る</a>

<!--
<br>テンプレート<br>

<h3><br><span style="color:blue;"></span></h3>

<p>
</p>
<p>
<a href="">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/">要約はこちら</a><br>
</p>

-->

    </body>
</html>