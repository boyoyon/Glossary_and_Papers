<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>Curriculum Learning</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 20px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
<style>
.container {
  display: flex;
  flex-wrap: wrap;
}
.column {
  flex: 0 25%; /* 幅を25%に設定して4列に */
  box-sizing: border-box;
  padding: 10px;
}
</style>

    </head>
    <body>
        <h1><center>Curriculum Learning (カリキュラム学習)</center></h1>
<a href="#latest">最新項目へ</a><br>

<h3>Curriculum learning (2009)<br><span style="color:blue;">カリキュラム学習</span></h3>

<p>
人間や動物は、例がランダムに提示されるのではなく、<strong><span style="color:magenta;">意味のある順序で整理され、徐々に多くの概念、そして徐々に複雑な概念を示すように提示されると、はるかによく学習する。
このような学習戦略を機械学習の文脈で定式化し、「カリキュラム学習」と呼ぶ。</span></strong>
</p>
<p>
論文は Google Scholar で検索すると pdf に辿り着ける<br>
</p>


<h3>Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning (2017)<br><span style="color:blue;">自動カリキュラム学習による内発的動機づけ目標探索プロセス</span></h3>

<p>
内発的動機付け目標探索プロセス（IMGEP）を形式化し、挑戦的なシミュレーション環境や現実世界のロボット環境で、ネストされたツール使用を含む多様で複雑なスキルを自律エージェントが発見できるようにするアーキテクチャであるActive Model Babbling（AMB）を導入した。<strong><span style="color:magenta;">AMBは、オブジェクト中心のモジュール性と能力進捗に基づいた内発的動機付けを活用することで、自動的な学習カリキュラムを生成する。</span></strong>
</p>
<p>
<a href="https://arxiv.org/abs/1708.02190">論文はこちら</a><br>
<a href="https://github.com/boyoyon/SeminalPapers/blob/main">要約はこちら</a><br>
</p>

<h3 id="latest">Hindsight experience replay (2017)<br><span style="color:blue;">後知恵経験再生</span></h3>

<p>
目標条件付き強化学習(Goal-Conditioned Reinforcement Learning)において非常に影響力のある手法である Hindsight Experience Replay (HER) を導入した。
<div class="styleBullet">
<ul>
<li>
・疎な報酬への対応は、強化学習 (RL) における最大の課題の 1 つある。
</li><li>
・Hindsight Experience Replay という新しい手法を導入し, 疎でバイナリの報酬からサンプル効率の高い学習を可能にし、複雑な報酬エンジニアリングの必要性を回避する。
</li><li>
・任意のオフポリシー RL アルゴリズムと組み合わせることができ、<strong><span style="color:magenta;">暗黙のカリキュラムの一形態と見なすことができる。</span></strong>
</li><li>
・ロボットアームで物体を操作するタスクでこのアプローチを実証する。
</li></ul></div>
</p>
<p>
<a href="https://arxiv.org/abs/1707.01495">論文はこちら</a><br>
<a href="https://github.com/boyoyon/SeminalPapers/blob/main">要約はこちら</a><br>
</p>

<a href="#">トップに戻る</a>

<!--
<br>テンプレート<br>

<h3><br><span style="color:blue;"></span></h3>

<p>
</p>
<p>
<a href="">論文はこちら</a><br>
<a href="">要約はこちら</a><br>
</p>

-->

    </body>
</html>