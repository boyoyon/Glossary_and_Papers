<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>Inductive Bias</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
<style>
.container {
  display: flex;
  flex-wrap: wrap;
}
.column {
  flex: 0 25%; /* 幅を25%に設定して4列に */
  box-sizing: border-box;
  padding: 10px;
}
</style>

    </head>
    <body>
        <h1><center>Inductive Bias (帰納バイアス)</center></h1>
<a href="#latest">最新項目へ</a><br>


<h3>The need for biases in learning generalizations (1980)<br><span style="color:blue;">一般化の学習におけるバイアスの必要性</span></h3>

<p>
機械学習における帰納的バイアスの概念を導入したもの。
『学習とは、過去の経験から一般化を行い、その経験に「関連する」新しい状況に対処する能力を伴う。新しい状況に対処するために必要な帰納的飛躍は、状況のある一般化を他の一般化よりも選択するための特定のバイアスがある場合にのみ可能となるように思われる･･･』
</p>
<p>
<a href="https://www.cs.cmu.edu/~tom/pubs/NeedForBias_1980.pdf">論文はこちら</a><br>
</p>


<h3>Relational inductive biases, deep learning, and graph networks (2018)<br><span style="color:blue;">関係性帰納バイアス、ディープラーニング、およびグラフネットワーク</span></h3>

<p>
グラフネットワークは強力な関係性帰納バイアスを組み込むことで、人工知能における人間のような組み合わせ的汎化を達成するための重要な一歩であると主張。
</p>
<p>
<a href="https://arxiv.org/abs/1806.01261">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/1806.01261v3">要約はこちら</a><br>
</p>


<h3>Generalization in diffusion models arises from geometry-adaptive harmonic representations (2023)<br><span style="color:blue;">拡散モデルにおける汎化は、幾何学適応型調和表現に由来する</span></h3>

<p>
幾何学適応型調和表現（GAHBs）を根本的な帰納的バイアスとして特定した。
</p>
<p>
<a href="https://arxiv.org/abs/2310.02557">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/2310.02557v3">要約はこちら</a><br>
</p>


<h3>Neural redshift: Random networks are not random functions (2024)<br><span style="color:blue;">Neural redshift: ランダムネットワークはランダム関数ではない</span></h3>

<p>
ReLU活性化関数が単純性バイアスにとって重要であり、活性化関数がネットワークの帰納的バイアスに影響を与える最も重要な要素であることを示した。
</p>
<p>
<a href="https://arxiv.org/abs/2403.02241">論文はこちら</a><br>
</p>

<h3>Do We Always Need the Simplicity Bias? Looking for Optimal Inductive Biases in the Wild (2025)<br><span style="color:blue;">我々は常に単純性バイアスを必要とするのか？野生における最適な帰納的バイアスを探して</span></h3>

<p>
ニューラルネットワークにとって単純性バイアスが普遍的に有益であるという広く信じられている見方を調査し、この研究は、カスタム活性化関数をメタ学習することで、帰納的バイアスを特定のタスクに適応させ、回帰および表形式データセットのパフォーマンスを向上させ、grokkingのような現象を排除できることを示している。
</p>
<p>
<a href="https://arxiv.org/abs/2503.10065">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/2503.10065v1">要約はこちら</a><br>
</p>


<h3 id="latest">Learning Model Successors (2025)<br><span style="color:blue;">後継モデルの学習</span></h3>

<p>
「モデル後続」を学習することで、難易度の進行に沿った帰納的汎化を形式化し可能にする新しいパラダイムである帰納的学習を提案している。
</p>
<p>
<a href="https://arxiv.org/abs/2502.00197">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/2502.00197v2">要約はこちら</a><br>
</p>

<a href="#">トップに戻る</a>

<!--
<br>テンプレート<br>

<h3><br><span style="color:blue;"></span></h3>

<p>
</p>
<p>
<a href="">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/">要約はこちら</a><br>
</p>

-->

    </body>
</html>