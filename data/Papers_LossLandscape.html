<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>Loss Landscape</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 20px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
<style>
.container {
  display: flex;
  flex-wrap: wrap;
}
.column {
  flex: 0 25%; /* 幅を25%に設定して4列に */
  box-sizing: border-box;
  padding: 10px;
}
</style>

    </head>
    <body>
        <h1><center>Loss Landscape (損失景観)</center></h1>
<a href="#latest">最新項目へ</a><br>

<!--●--> <h3>Visualizing the Loss Landscape of Neural Nets (2017)<br><span style="color:blue;">ニューラルネットワークの損失景観の可視化</span></h3>

<p>
<div class="styleBullet">
<ul><li>
・<strong><span style="color:magenta;">高次元の損失関数を、2次元平面上にプロットする革新的な可視化手法を提案した</span></strong>。これにより、学習パラメータ空間の「地形」を視覚的に捉えることが可能になった。
</li><li>
・可視化の結果、最適解（最小値）の形状が、モデルの汎化性能に大きく影響することを発見した。この論文は、<strong><span style="color:magenta;">「平坦な最小値（flat minima）」にたどり着いたモデルは、より優れた汎化性能を持つことを視覚的に示した</span></strong>。
</li><li>
・ResNetなどのネットワークで使われる<strong><span style="color:magenta;">「スキップコネクション」が、損失ランドスケープをよりスムーズにする効果があることも明らかにした</span></strong>。
</li></ul></div> 
</p>
<p>
<a href="https://arxiv.org/abs/1712.09913">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/1712.09913v3">要約はこちら</a><br>
</p>


<!--●--> <h3>Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs (2018)<br><span style="color:blue;">損失面、モード接続性、およびDNNの高速アンサンブル</span></h3>

<p>
<div class="styleBullet">
<ul><li>
・<strong><span style="color:magenta;">異なる初期値から学習をスタートさせた複数のモデルがたどり着く局所最小値（「モード」）が、低損失の経路で繋がっている現象を発見した</span></strong>。
</li><li>
・この発見に基づき、<strong><span style="color:magenta;">学習済みの複数のモデルパラメータを線形補間するだけで、低損失を維持しながら、より優れた汎化性能を持つアンサンブルモデルを作成できることを示した</span></strong>。これは、モデルアンサンブルを効率的に行う新しい方法を提示した。
</li></ul></div>
</p>
<p>
<a href="https://arxiv.org/abs/1802.10026">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/1802.10026v4">要約はこちら</a><br>
</p>


<!--●--> <h3>Sharpness-Aware Minimization for Efficiently Improving Generalization (2020)<br><span style="color:blue;">シャープネスを考慮した最小化による効率的な一般化の改善</span></h3>

<p>
<div class="styleBullet">
<ul><li>
・平坦な最小値に収束させることを目的とした新しい最適化手法「SAM（Sharpness-Aware Minimization）」を提案した。
</li><li>
・局所的な損失の最小化だけでなく、その近傍における「最悪の」損失も同時に最小化することで、より平坦な最小値を見つけることを試みた。
</li><li>
・SAMは、既存の最適化手法に比べて、さまざまなデータセットで優れた汎化性能を示すことが実証され、損失ランドスケープ研究の応用可能性を大きく広げた。
</li></ul></div>
</p>
<p>
<a href="https://arxiv.org/abs/2010.01412">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/2010.01412v3">要約はこちら</a><br>
</p>


<a href="#">トップに戻る</a>

<!--
<br>テンプレート<br>

<!--●-- <h3><br><span style="color:blue;"></span></h3>

<p>
</p>
<p>
<a href="">論文はこちら</a><br>
<a href="https://www.alphaxiv.org/ja/overview/">要約はこちら</a><br>
</p>



<div class="styleBullet">
<ul><li>

</li><br><li>

</li></ul></div>

-->

    </body>
</html>